# Default configuration file for the Linux (deb/rpm) and Windows MSI collector packages
# If the collector is installed without the Linux/Windows installer script, the following
# environment variables are required to be manually defined or configured below:
# - SPLUNK_ACCESS_TOKEN: The Splunk access token to authenticate requests
# - SPLUNK_API_URL: The Splunk API URL, e.g. https://api.us0.signalfx.com
# - SPLUNK_BUNDLE_DIR: The path to the Smart Agent bundle, e.g. /usr/lib/splunk-otel-collector/agent-bundle
# - SPLUNK_COLLECTD_DIR: The path to the collectd config directory for the Smart Agent, e.g. /usr/lib/splunk-otel-collector/agent-bundle/run/collectd
# - SPLUNK_HEC_TOKEN: The Splunk HEC authentication token
# - SPLUNK_HEC_URL: The Splunk HEC endpoint URL, e.g. https://ingest.us0.signalfx.com/v1/log
# - SPLUNK_INGEST_URL: The Splunk ingest URL, e.g. https://ingest.us0.signalfx.com
# - SPLUNK_TRACE_URL: The Splunk trace endpoint URL, e.g. https://ingest.us0.signalfx.com/v2/trace

extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  http_forwarder:
    ingress:
      endpoint: 0.0.0.0:6060
    egress:
      endpoint: "${SPLUNK_API_URL}"
      # Use instead when sending to gateway
      #endpoint: "${SPLUNK_GATEWAY_URL}"
  smartagent:
    bundleDir: "${SPLUNK_BUNDLE_DIR}"
    collectd:
      configDir: "${SPLUNK_COLLECTD_DIR}"
  zpages:
    #endpoint: 0.0.0.0:55679
  memory_ballast:
    # In general, the ballast should be set to 1/3 of the collector's memory, the limit
    # should be 90% of the collector's memory.
    # The simplest way to specify the ballast size is set the value of SPLUNK_BALLAST_SIZE_MIB env variable.
    size_mib: ${SPLUNK_BALLAST_SIZE_MIB}

receivers:
  hostmetrics:
    collection_interval: 10s
    scrapers:
      cpu:
      disk:
      filesystem:
      memory:
      network:
      # System load average metrics https://en.wikipedia.org/wiki/Load_(computing)
      load:
      # Paging/Swap space utilization and I/O metrics
      paging:
      # Aggregated system process count metrics
      processes:
      # System processes metrics, disabled by default
      # process:
  otlp/jenkins:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:55681
  # This section is used to collect the OpenTelemetry Collector metrics
  # Even if just a Splunk APM customer, these metrics are included
  prometheus/internal:
    config:
      scrape_configs:
      - job_name: 'otel-collector'
        scrape_interval: 10s
        static_configs:
        - targets: ['0.0.0.0:8888']
        metric_relabel_configs:
          - source_labels: [ __name__ ]
            regex: '.*grpc_io.*'
            action: drop
  signalfx:
    endpoint: 0.0.0.0:9943

# Connectors allow creating telemetry (metrics, logs, etc) from other opentelemetry data
connectors:
  # spanmetrics connector will pull span metrics out of our trace data with dimensions for pipeline id, step name, step type.
  # Any other dimensions (span tags) can also be added here to put those dimensions onto the spanmetrics
  # https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/f195900aa20617dca2370df638f4776a0ffc160e/connector/spanmetricsconnector
  spanmetrics/jenkins:
    histogram:
      explicit:
        buckets: [1ms, 10ms, 100ms]
    dimensions:
      - name: ci.pipeline.id
        default: None
      - name: jenkins.pipeline.step.name
        default: None
      - name: jenkins.pipeline.step.type
        default: None
    dimensions_cache_size: 1000
    namespace: jenkins      

# Processors allow you to manipulate opentelemetry data in transit
processors:
  # Transform processor to enrich jenkins span data
  transform/jenkinstrace:
    trace_statements:
      - context: span
        statements:
          - set(attributes["jenkins.pipeline.step.name"], name) where attributes["jenkins.pipeline.step.name"] == nil  
          - set(attributes["jenkins.pipeline.step.type"], name) where attributes["jenkins.pipeline.step.type"] == nil
  
  batch:

  # Enabling the memory_limiter is strongly recommended for every pipeline.
  memory_limiter:
    check_interval: 2s
    limit_mib: ${SPLUNK_MEMORY_LIMIT_MIB}
  
  # detect if the collector is running on a cloud system
  # important for creating unique cloud provider dimensions
  resourcedetection:
    detectors: [system, gce, ecs, ec2, azure]
    override: false

  # Same as above but overrides resource attributes set by receivers
  resourcedetection/internal:
    detectors: [system, gce, ecs, ec2, azure]
    override: true

  # Add a tag to spans missing it.
  attributes:
    actions:
    - key: environment
      value: test
      action: upsert



exporters:
  # Traces
  sapm:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    endpoint: "${SPLUNK_TRACE_URL}"
  # Metrics + Events
  signalfx:
    access_token: "${SPLUNK_ACCESS_TOKEN}"
    api_url: "${SPLUNK_API_URL}"
    ingest_url: "${SPLUNK_INGEST_URL}"
    sync_host_metadata: true
    correlation:
  # Send to gateway
  otlp:
    endpoint: "${SPLUNK_GATEWAY_URL}:4317"
    insecure: true
  # Debug
  logging:
    loglevel: debug

service:
  extensions: [health_check, http_forwarder, zpages, memory_ballast]
  pipelines:
    traces:
      receivers: [otlp/jenkins]
      processors:
      - transform/jenkinstrace
      - memory_limiter
      - attributes  
      - batch
      - resourcedetection
      exporters: [sapm, signalfx, spanmetrics/jenkins]
      # Use instead when sending to gateway
      #exporters: [otlp, signalfx, spanmetrics/jenkins]
    metrics:
      receivers: [hostmetrics, otlp, signalfx, spanmetrics/jenkins]
      processors: [memory_limiter, batch, resourcedetection]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]
    metrics/internal:
      receivers: [prometheus/internal]
      processors: [memory_limiter, batch, resourcedetection/internal]
      exporters: [signalfx]
      # Use instead when sending to gateway
      #exporters: [otlp]